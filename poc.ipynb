{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "poc.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nalsadi/DefImageRegMnist/blob/main/poc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBFigDjXNJH-"
      },
      "source": [
        "A simple example for deep-learning-based non-rigid image registration\n",
        "with the MNIST dataset.\n",
        "\n",
        "**README:** If the below error occurs, run the whole notebook again (Ctrl+F9).\n",
        "\n",
        "\n",
        "```\n",
        "ValueError: tf.function-decorated function tried to create variables on non-first call.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hIrk9O_i0FQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ec663f-ed9a-4133-cd9b-8a0d800c26b2"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "print(tf.__version__)\n",
        "print(tf.keras.backend.image_data_format())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n",
            "channels_last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf2ZStszjDXr"
      },
      "source": [
        "Loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D70TmGfg_MvB"
      },
      "source": [
        "@tf.function\n",
        "def mse_loss(static, moving):\n",
        "    \"\"\"Computes the mean squared error (MSE) loss.\n",
        "\n",
        "    Currently, only 4-D inputs are supported.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    static : tf.Tensor, shape (N, H, W, C)\n",
        "        The static image to which the moving image is aligned.\n",
        "    moving : tf.Tensor, shape (N, H, W, C)\n",
        "        The moving image, the same shape as the static image.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    loss : tf.Tensor, shape ()\n",
        "        Mean squared error between the static and the moving images,\n",
        "        averaged over the batch.\n",
        "    \"\"\"\n",
        "    loss = tf.reduce_mean(tf.square(moving - static))  # shape ()\n",
        "    return loss"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDSIHBx5jQoE"
      },
      "source": [
        "@tf.function\n",
        "def ncc_loss(static, moving):\n",
        "    \"\"\"Computes the normalized cross-correlation (NCC) loss.\n",
        "\n",
        "    Currently, only 4-D inputs are supported.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    static : tf.Tensor, shape (N, H, W, C)\n",
        "        The static image to which the moving image is aligned.\n",
        "    moving : tf.Tensor, shape (N, H, W, C)\n",
        "        The moving image, the same shape as the static image.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    loss : tf.Tensor, shape ()\n",
        "        Normalized cross-correlation loss between the static and the\n",
        "        moving images, averaged over the batch. Range is [-1.0, 1.0].\n",
        "        The best value is -1 (perfect match) and the worst is 1.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] `Wikipedia entry for the Cross-correlation\n",
        "           <https://en.wikipedia.org/wiki/Cross-correlation>`_\n",
        "    \"\"\"\n",
        "    eps = tf.constant(1e-9, 'float32')\n",
        "\n",
        "    static_mean = tf.reduce_mean(static, axis=[1, 2], keepdims=True)\n",
        "    moving_mean = tf.reduce_mean(moving, axis=[1, 2], keepdims=True)\n",
        "    # shape (N, 1, 1, C)\n",
        "\n",
        "    static_std = tf.math.reduce_std(static, axis=[1, 2], keepdims=True)\n",
        "    moving_std = tf.math.reduce_std(moving, axis=[1, 2], keepdims=True)\n",
        "    # shape (N, 1, 1, C)\n",
        "\n",
        "    static_hat = (static - static_mean)/(static_std + eps)\n",
        "    moving_hat = (moving - moving_mean)/(moving_std + eps)\n",
        "    # shape (N, H, W, C)\n",
        "\n",
        "    ncc = tf.reduce_mean(static_hat * moving_hat)  # shape ()\n",
        "    loss = -ncc\n",
        "    return loss"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WuPTnpejscE"
      },
      "source": [
        "Define the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yfqxog30zvit"
      },
      "source": [
        "def simple_cnn(input_shape=(32, 32, 2)):\n",
        "    \"\"\"Creates a 2-D convolutional encoder-decoder network.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_shape : sequence of ints, optional\n",
        "        Input data shape of the form (H, W, C). Default is (32, 32, 2).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model\n",
        "        An instance of Keras' Model class.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    Given a concatenated pair of static and moving images as input, the\n",
        "    CNN computes a dense displacement field that is used to warp the\n",
        "    moving image to match with the static image.\n",
        "\n",
        "    The number of channels in the output (displacement field) is equal\n",
        "    to the dimensionality of the input data. For 3-D volumes, it is 3,\n",
        "    and for 2-D images, it is 2. The first channel comprises\n",
        "    displacement in the x-direction and the second comprises\n",
        "    displacement in the y-direction.\n",
        "    \"\"\"\n",
        "    out_channels = 2\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # encoder\n",
        "    x = layers.Conv2D(32, kernel_size=3, strides=2, padding='same',\n",
        "                      activation='relu')(inputs)            # 32 --> 16\n",
        "    x = layers.BatchNormalization()(x)                      # 16\n",
        "    x = layers.Conv2D(32, kernel_size=3, strides=1, padding='same',\n",
        "                      activation='relu')(x)                 # 16\n",
        "    x = layers.BatchNormalization()(x)                      # 16\n",
        "    x = layers.MaxPool2D(pool_size=2)(x)                    # 16 --> 8\n",
        "    x = layers.Conv2D(64, kernel_size=3, strides=1, padding='same',\n",
        "                      activation='relu')(x)                 # 8\n",
        "    x = layers.BatchNormalization()(x)                      # 8\n",
        "    x = layers.Conv2D(64, kernel_size=3, strides=1, padding='same',\n",
        "                      activation='relu')(x)                 # 8\n",
        "    x = layers.BatchNormalization()(x)                      # 8\n",
        "    x = layers.MaxPool2D(pool_size=2)(x)                    # 8 --> 4\n",
        "    x = layers.Conv2D(128, kernel_size=3, strides=1, padding='same',\n",
        "                      activation='relu')(x)                 # 4\n",
        "    x = layers.BatchNormalization()(x)                      # 4\n",
        "\n",
        "    # decoder\n",
        "    x = layers.Conv2DTranspose(64, kernel_size=2, strides=2,\n",
        "                               padding='same')(x)           # 4 --> 8\n",
        "    x = layers.Conv2D(64, kernel_size=3, strides=1, padding='same',\n",
        "                      activation='relu')(x)                 # 8\n",
        "    x = layers.BatchNormalization()(x)                      # 8\n",
        "    x = layers.Conv2DTranspose(32, kernel_size=2, strides=2,\n",
        "                               padding='same')(x)           # 8 --> 16\n",
        "    x = layers.Conv2D(32, kernel_size=3, strides=1, padding='same',\n",
        "                      activation='relu')(x)                 # 16\n",
        "    x = layers.BatchNormalization()(x)                      # 16\n",
        "    x = layers.Conv2DTranspose(16, kernel_size=2, strides=2,\n",
        "                               padding='same')(x)           # 16 --> 32\n",
        "    x = layers.Conv2D(16, kernel_size=3, strides=1, padding='same',\n",
        "                      activation='relu')(x)                 # 32\n",
        "    x = layers.BatchNormalization()(x)                      # 32\n",
        "    x = layers.Conv2D(out_channels, kernel_size=1, strides=1,\n",
        "                      padding='same')(x)                    # 32\n",
        "\n",
        "    # Create the model.\n",
        "    model = tf.keras.Model(inputs, x, name='simple_cnn')\n",
        "    return model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNwVzBzM1ytM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32c50a2d-05f4-46a7-b47f-7dc1fd8e683d"
      },
      "source": [
        "model = simple_cnn()\n",
        "model.summary()\n",
        "# tf.keras.utils.plot_model(model, show_shapes=True, dpi=50)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"simple_cnn\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 32, 32, 2)]       0         \n",
            "                                                                 \n",
            " conv2d_27 (Conv2D)          (None, 16, 16, 32)        608       \n",
            "                                                                 \n",
            " batch_normalization_24 (Bat  (None, 16, 16, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_28 (Conv2D)          (None, 16, 16, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_25 (Bat  (None, 16, 16, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 8, 8, 32)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_29 (Conv2D)          (None, 8, 8, 64)          18496     \n",
            "                                                                 \n",
            " batch_normalization_26 (Bat  (None, 8, 8, 64)         256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_30 (Conv2D)          (None, 8, 8, 64)          36928     \n",
            "                                                                 \n",
            " batch_normalization_27 (Bat  (None, 8, 8, 64)         256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 4, 4, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_31 (Conv2D)          (None, 4, 4, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_28 (Bat  (None, 4, 4, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_transpose_9 (Conv2DT  (None, 8, 8, 64)         32832     \n",
            " ranspose)                                                       \n",
            "                                                                 \n",
            " conv2d_32 (Conv2D)          (None, 8, 8, 64)          36928     \n",
            "                                                                 \n",
            " batch_normalization_29 (Bat  (None, 8, 8, 64)         256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_transpose_10 (Conv2D  (None, 16, 16, 32)       8224      \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " conv2d_33 (Conv2D)          (None, 16, 16, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_30 (Bat  (None, 16, 16, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_transpose_11 (Conv2D  (None, 32, 32, 16)       2064      \n",
            " Transpose)                                                      \n",
            "                                                                 \n",
            " conv2d_34 (Conv2D)          (None, 32, 32, 16)        2320      \n",
            "                                                                 \n",
            " batch_normalization_31 (Bat  (None, 32, 32, 16)       64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_35 (Conv2D)          (None, 32, 32, 2)         34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 232,514\n",
            "Trainable params: 231,650\n",
            "Non-trainable params: 864\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xY82_rSyqCYG"
      },
      "source": [
        "\n",
        "\n",
        "Differntiable image sampling\n",
        "\n",
        "\n",
        "References:\n",
        "\n",
        "\n",
        "1.   https://github.com/tensorflow/models/blob/master/research/transformer/spatial_transformer.py\n",
        "2.   Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. \"Spatial\n",
        "    transformer networks.\" Advances in neural information processing\n",
        "    systems. 2015. https://arxiv.org/pdf/1506.02025.pdf\n",
        "3.   *Spatial* Transformer Networks by Kushagra Bhatnagar https://link.medium.com/0b2OrmqVO5\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk9WTAQajpsX"
      },
      "source": [
        "@tf.function\n",
        "def grid_sample(moving, grid):\n",
        "    \"\"\"Given a moving image and a sampling grid as input, computes the\n",
        "    transformed image by sampling the moving image at locations given by\n",
        "    the grid.\n",
        "\n",
        "    Currently, only 2-D images, i.e., 4-D inputs are supported.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    moving : tf.Tensor, shape (N, H, W, C)\n",
        "        The moving image.\n",
        "    grid : tf.Tensor, shape (N, H, W, C)\n",
        "        A tensor of sampling points (x, y). The x and y values should be\n",
        "        normalized to [-1.0, 1.0] range.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    moved : tf.Tensor, shape (N, H, W, C)\n",
        "        The transformed image.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    Let M be the moving image of shape (H, W, C), T be the transformed\n",
        "    image of the same shape and G be the 2-D sampling grid of shape\n",
        "    (H, W, 2). The value of T at a location (x, y) is T[y, x, :] =\n",
        "    M[y', x', :] where [x', y'] = G[y, x, :].\n",
        "\n",
        "    Further, [x', y'] = [x + dx, y + dy] where [dx, dy] are the\n",
        "    displacements outputted by the CNN. When dx and dy are 0, the\n",
        "    sampling grid G is a regular grid and the transformed image is the\n",
        "    same as the moving image.\n",
        "\n",
        "    Since the sampling point (x + dx, y + dy) can be non-integral, the\n",
        "    value M[y', x'] is calculated using bi-linear interpolation.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] `Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. \"Spatial\n",
        "        transformer networks.\" Advances in neural information processing\n",
        "        systems. 2015. <https://arxiv.org/abs/1506.02025>`_\n",
        "    .. [2] `TensorFlow implementation of spatial transformer networks.\n",
        "        <https://github.com/tensorflow/models/tree/master/research/transformer>`_\n",
        "    .. [3] `Spatial Transformer Networks by Kushagra Bhatnagar\n",
        "        <https://link.medium.com/0b2OrmqVO5>`_\n",
        "    \"\"\"\n",
        "    nb, nh, nw, nc = moving.shape\n",
        "\n",
        "    x = grid[..., 0]  # shape (N, H, W)\n",
        "    y = grid[..., 1]\n",
        "    x = tf.cast(x, 'float32')\n",
        "    y = tf.cast(y, 'float32')\n",
        "\n",
        "    # Scale x and y from [-1.0, 1.0] to [0, W] and [0, H] respectively.\n",
        "    x = (x + 1.0) * 0.5 * tf.cast(nw, 'float32')\n",
        "    y = (y + 1.0) * 0.5 * tf.cast(nh, 'float32')\n",
        "\n",
        "    y_max = tf.cast(nh - 1, 'int32')\n",
        "    x_max = tf.cast(nw - 1, 'int32')\n",
        "    zero = tf.constant(0, 'int32')\n",
        "\n",
        "    # The value at (x, y) is a weighted average of the values at the\n",
        "    # four nearest integer locations: (x0, y0), (x1, y0), (x0, y1) and\n",
        "    # (x1, y1) where x0 = floor(x), x1 = ceil(x).\n",
        "    x0 = tf.cast(tf.floor(x), 'int32')\n",
        "    x1 = x0 + 1\n",
        "    y0 = tf.cast(tf.floor(y), 'int32')\n",
        "    y1 = y0 + 1\n",
        "\n",
        "    # Make sure indices are within the boundaries of the image.\n",
        "    x0 = tf.clip_by_value(x0, zero, x_max)\n",
        "    x1 = tf.clip_by_value(x1, zero, x_max)\n",
        "    y0 = tf.clip_by_value(y0, zero, y_max)\n",
        "    y1 = tf.clip_by_value(y1, zero, y_max)\n",
        "\n",
        "    # Collect indices of the four corners.\n",
        "    b = tf.ones_like(x0) * tf.reshape(tf.range(nb), [nb, 1, 1])\n",
        "    idx_a = tf.stack([b, y0, x0], axis=-1)  # all top-left corners\n",
        "    idx_b = tf.stack([b, y1, x0], axis=-1)  # all bottom-left corners\n",
        "    idx_c = tf.stack([b, y0, x1], axis=-1)  # all top-right corners\n",
        "    idx_d = tf.stack([b, y1, x1], axis=-1)  # all bottom-right corners\n",
        "    # shape (N, H, W, 3)\n",
        "\n",
        "    # Collect values at the corners.\n",
        "    moving_a = tf.gather_nd(moving, idx_a)  # all top-left values\n",
        "    moving_b = tf.gather_nd(moving, idx_b)  # all bottom-left values\n",
        "    moving_c = tf.gather_nd(moving, idx_c)  # all top-right values\n",
        "    moving_d = tf.gather_nd(moving, idx_d)  # all bottom-right values\n",
        "    # shape (N, H, W, C)\n",
        "\n",
        "    x0_f = tf.cast(x0, 'float32')\n",
        "    x1_f = tf.cast(x1, 'float32')\n",
        "    y0_f = tf.cast(y0, 'float32')\n",
        "    y1_f = tf.cast(y1, 'float32')\n",
        "\n",
        "    # Calculate the weights.\n",
        "    wa = tf.expand_dims((x1_f - x) * (y1_f - y), axis=-1)\n",
        "    wb = tf.expand_dims((x1_f - x) * (y - y0_f), axis=-1)\n",
        "    wc = tf.expand_dims((x - x0_f) * (y1_f - y), axis=-1)\n",
        "    wd = tf.expand_dims((x - x0_f) * (y - y0_f), axis=-1)\n",
        "\n",
        "    # Calculate the weighted sum.\n",
        "    moved = tf.add_n([wa * moving_a, wb * moving_b, wc * moving_c,\n",
        "                      wd * moving_d])\n",
        "    return moved"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgICzWJbtA0b"
      },
      "source": [
        "@tf.function\n",
        "def regular_grid(shape):\n",
        "    \"\"\"Returns a batch of 2-D regular grids.\n",
        "\n",
        "    Currently, only 2-D regular grids are supported.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    shape : sequence of ints, shape (3, )\n",
        "        The desired regular grid shape of the form (N, H, W).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    grid : tf.Tensor, shape (N, H, W, 2)\n",
        "        A batch of 2-D regular grids, values normalized to [-1.0, 1.0]\n",
        "        range.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    Sampling using the regular grid is an identity transformation, i.e.,\n",
        "    it results in the same input and output images.\n",
        "\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] `NumPy, \"numpy.meshgrid\"\n",
        "        <https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html>`_\n",
        "    .. [2] `NumPy, \"numpy.indices\"\n",
        "        <https://numpy.org/doc/stable/reference/generated/numpy.indices.html>`_\n",
        "    \"\"\"\n",
        "    nb, nh, nw = shape\n",
        "\n",
        "    x = tf.linspace(-1.0, 1.0, nw)  # shape (W, )\n",
        "    y = tf.linspace(-1.0, 1.0, nh)  # shape (H, )\n",
        "\n",
        "    X, Y = tf.meshgrid(x, y)  # shape (H, W), both X and Y\n",
        "\n",
        "    grid = tf.stack([X, Y], axis=-1)\n",
        "    grid = tf.expand_dims(grid, axis=0)  # shape (1, H, W, 2)\n",
        "\n",
        "    # Repeat the grids along the batch dim.\n",
        "    multiples = tf.constant([nb, 1, 1, 1], tf.int32)\n",
        "    grid = tf.tile(grid, multiples)\n",
        "    return grid"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng5Zjq7TuI_4"
      },
      "source": [
        "Training and testing functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PELWSvTuMIt"
      },
      "source": [
        "@tf.function\n",
        "def train_step(model, moving, static, criterion, optimizer):\n",
        "    \"\"\"A generic training procedure for one iteration.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model\n",
        "        A convolutional encoder-decoder network.\n",
        "    moving : tf.Tensor, shape (N, H, W, C)\n",
        "        A batch of moving images.\n",
        "    static : tf.Tensor, shape (1, H, W, C)\n",
        "        The static image.\n",
        "    criterion\n",
        "        The loss function.\n",
        "    optimizer\n",
        "        An optimzer.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    loss : tf.Tensor, shape ()\n",
        "        The average loss for the batch.\n",
        "    \"\"\"\n",
        "    nb, nh, nw, nc = moving.shape\n",
        "\n",
        "    # Repeat the static image along the batch dim.\n",
        "    multiples = tf.constant([nb, 1, 1, 1], tf.int32)\n",
        "    static = tf.tile(static, multiples)\n",
        "\n",
        "    # Define the GradientTape context for automatic differentiation.\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Get the deformation field\n",
        "        inputs = tf.concat([moving, static], axis=-1)\n",
        "        deformation = model(inputs)\n",
        "\n",
        "        # Compute the new sampling grid.\n",
        "        grid = regular_grid([nb, nh, nw])\n",
        "        grid_new = grid + deformation\n",
        "        grid_new = tf.clip_by_value(grid_new, -1, 1)\n",
        "\n",
        "        # Sample the moving image using the new sampling grid.\n",
        "        moved = grid_sample(moving, grid_new)\n",
        "\n",
        "        # Compute the loss.\n",
        "        loss = criterion(moved, static)\n",
        "\n",
        "    # Compute gradients.\n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    # Update the trainable parameters.\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3Zx1obPAL5e"
      },
      "source": [
        "@tf.function\n",
        "def test_step(model, moving, static, criterion):\n",
        "    \"\"\"A generic testing procedure.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model\n",
        "        A convolutional encoder-decoder network.\n",
        "    moving : tf.Tensor, shape (N, H, W, C)\n",
        "        A batch of moving images.\n",
        "    static : tf.Tensor, shape (1, H, W, C)\n",
        "        The static image.\n",
        "    criterion\n",
        "        The loss function.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    loss : tf.Tensor, shape ()\n",
        "        The average loss for the batch.\n",
        "    \"\"\"\n",
        "    nb, nh, nw, nc = moving.shape\n",
        "\n",
        "    # Repeat the static image along the batch dim.\n",
        "    multiples = tf.constant([nb, 1, 1, 1], tf.int32)\n",
        "    static = tf.tile(static, multiples)\n",
        "\n",
        "    # Get the deformation field.\n",
        "    inputs = tf.concat([moving, static], axis=-1)\n",
        "    deformation = model(inputs, training=False)\n",
        "\n",
        "    # Compute the new sampling grid.\n",
        "    grid = regular_grid([nb, nh, nw])\n",
        "    grid_new = grid + deformation\n",
        "    grid_new = tf.clip_by_value(grid_new, -1, 1)\n",
        "\n",
        "    # Sample the moving image using the new sampling grid.\n",
        "    moved = grid_sample(moving, grid_new)\n",
        "\n",
        "    # Compute the loss.\n",
        "    loss = criterion(moved, static)\n",
        "    return loss"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZL5-P8jnxOd"
      },
      "source": [
        "Data loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TALMZdklmlOu"
      },
      "source": [
        "def load_data(label=2):\n",
        "    \"\"\"Loads the MNIST dataset and preprocesses it: scales to [0.0, 1.0]\n",
        "    range, resizes the images from (28, 28) to (32, 32) and filters the\n",
        "    dataset to keep images of just one class.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    label : {2, 0, 1, 3, 4, 5, 6, 7, 8, 9}, default 2\n",
        "        The class of images to train and test on.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    (x_train, x_test) : tuple of ndarrays\n",
        "        NumPy arrays of training and testing images.\n",
        "    \"\"\"\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "    # Discard digits which are not equal to label.\n",
        "    ids_train = np.where(y_train == label)\n",
        "    ids_test = np.where(y_test == label)\n",
        "\n",
        "    x_train = x_train[ids_train]\n",
        "    x_test = x_test[ids_test]\n",
        "\n",
        "    # Scale the image to [0, 1] range.\n",
        "    x_train = x_train.astype(np.float32) / 255.0\n",
        "    x_test = x_test.astype(np.float32) / 255.0\n",
        "\n",
        "    # Add the channel dim at the end. (N, H, W) --> (N, H, W, 1)\n",
        "    x_train = x_train[..., None]\n",
        "    x_test = x_test[..., None]\n",
        "\n",
        "    # Resize images from (28, 28) to (32, 32).\n",
        "    x_train = tf.image.resize(x_train, (32, 32))\n",
        "    x_test = tf.image.resize(x_test, (32, 32))\n",
        "    return x_train, x_test"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_guvhDzon42"
      },
      "source": [
        "Sample results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7F3RdkZUoolL"
      },
      "source": [
        "def plot_images(model, moving, static):\n",
        "    \"\"\"Visualize some images after training.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model\n",
        "        The trained model.\n",
        "    moving : tf.Tensor, shape (N, H, W, C)\n",
        "        A batch of moving images.\n",
        "    static : tf.Tensor, shape (1, H, W, C)\n",
        "        The static image.\n",
        "    \"\"\"\n",
        "    nb, nh, nw, nc = moving.shape\n",
        "\n",
        "    # Repeat the static image along the batch dim.\n",
        "    multiples = tf.constant([nb, 1, 1, 1], tf.int32)\n",
        "    static = tf.tile(static, multiples)\n",
        "\n",
        "    # Get the deformation fields for the batch.\n",
        "    inputs = tf.concat([moving, static], axis=-1)\n",
        "    deformation = model(inputs, training=False)\n",
        "\n",
        "    # Compute the new sampling grids.\n",
        "    grid = regular_grid([nb, nh, nw])\n",
        "    grid_new = grid + deformation\n",
        "    grid_new = tf.clip_by_value(grid_new, -1, 1)\n",
        "\n",
        "    # Sample the moving images using the new sampling grids.\n",
        "    moved = grid_sample(moving, grid_new)\n",
        "\n",
        "    # Convert the tensors to 8-bit images.\n",
        "    moved = moved.numpy().squeeze(axis=-1) * 255.0\n",
        "    moved = moved.astype(np.uint8)\n",
        "    moving = moving.numpy().squeeze(axis=-1) * 255.0\n",
        "    moving = moving.astype(np.uint8)\n",
        "    static = static.numpy().squeeze(axis=-1) * 255.0\n",
        "    static = static.astype(np.uint8)\n",
        "\n",
        "    # Plot images.\n",
        "    fig = plt.figure(figsize=(3 * 1.7, nb * 1.7))\n",
        "    titles_list = ['Static', 'Moved', 'Moving']\n",
        "    images_list = [static, moved, moving]\n",
        "    for i in range(nb):\n",
        "        for j in range(3):\n",
        "            ax = fig.add_subplot(nb, 3, i * 3 + j + 1)\n",
        "            if i == 0:\n",
        "                ax.set_title(titles_list[j], fontsize=20)\n",
        "            ax.set_axis_off()\n",
        "            ax.imshow(images_list[j][i], cmap='gray')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjQIKGrMGo2S"
      },
      "source": [
        "def main(args):\n",
        "\n",
        "    # Load preprocessed training and testing data.\n",
        "    x_train, x_test = load_data(label=args.label)\n",
        "\n",
        "    # Randomly select an image as the static image from the test set.\n",
        "    # idx = np.random.randint(x_test.shape[0])\n",
        "    # static = tf.expand_dims(x_test[idx], axis=0)\n",
        "    static = tf.expand_dims(x_test[0], axis=0)\n",
        "\n",
        "    # Select some images from the test set to show sample results.\n",
        "    # ids = tf.constant(np.random.choice(x_test.shape[0], replace=False,\n",
        "    #                                    size=args.num_samples))\n",
        "    # x_sample = tf.gather(x_test, ids)\n",
        "    x_sample = x_test[:args.num_samples]\n",
        "\n",
        "    # Shuffle and batch the dataset.\n",
        "    from_tensor_slices = tf.data.Dataset.from_tensor_slices\n",
        "    # x_train = from_tensor_slices(x_train).shuffle(10000).batch(args.batch_size)\n",
        "    # x_test = from_tensor_slices(x_test).shuffle(10000).batch(args.batch_size)\n",
        "    x_train = from_tensor_slices(x_train).batch(args.batch_size)\n",
        "    x_test = from_tensor_slices(x_test).batch(args.batch_size)\n",
        "\n",
        "    # Create a model instance.\n",
        "    model = simple_cnn(input_shape=(32, 32, 2))\n",
        "\n",
        "    # Select optimizer and loss function.\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate=args.lr)\n",
        "    criterion = ncc_loss # normalized_cross_correlation_loss()  # or mse_loss\n",
        "\n",
        "    # Define the metrics to track training and testing losses.\n",
        "    m_train = tf.keras.metrics.Mean(name='loss_train')\n",
        "    m_test = tf.keras.metrics.Mean(name='loss_test')\n",
        "\n",
        "    # Train and evaluate the model.\n",
        "    for epoch in range(args.epochs):\n",
        "        m_train.reset_states()\n",
        "        m_test.reset_states()\n",
        "        for i, moving in enumerate(x_train):\n",
        "            loss_train = train_step(model, moving, static, criterion,\n",
        "                                    optimizer)\n",
        "            m_train.update_state(loss_train)\n",
        "\n",
        "        for i, moving in enumerate(x_test):\n",
        "            loss_test = test_step(model, moving, static, criterion)\n",
        "            m_test.update_state(loss_test)\n",
        "\n",
        "        print('Epoch: %3d/%d\\tTrain Loss: %.6f\\tTest Loss: %.6f'\n",
        "              % (epoch + 1, args.epochs, m_train.result(), m_test.result()))\n",
        "    print('\\n')\n",
        "\n",
        "    # Show sample results.\n",
        "    plot_images(model, x_sample, static)\n",
        "\n",
        "    # Save the trained model.\n",
        "    if args.save_model:\n",
        "        model.save('saved_models/simple_cnn')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHnyCw561Q15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "outputId": "d26db5bf-5c20-4ab6-a883-98d8150a7a35"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    class Args():\n",
        "      batch_size = 8\n",
        "      epochs = 5\n",
        "      lr = 0.004\n",
        "      label = 7  # which digit images to train on?\n",
        "      num_samples = 5  # number of sample results to show\n",
        "      save_model = False\n",
        "    \n",
        "    args = Args()\n",
        "    main(args)\n",
        "    "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:   1/5\tTrain Loss: -0.575568\tTest Loss: -0.588673\n",
            "Epoch:   2/5\tTrain Loss: -0.589912\tTest Loss: -0.603277\n",
            "Epoch:   3/5\tTrain Loss: -0.607063\tTest Loss: -0.624918\n",
            "Epoch:   4/5\tTrain Loss: -0.630076\tTest Loss: -0.652182\n",
            "Epoch:   5/5\tTrain Loss: -0.668358\tTest Loss: -0.700291\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 367.2x612 with 15 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}